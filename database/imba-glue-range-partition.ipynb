{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 10\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nfrom awsglue.dynamicframe import DynamicFrame\nfrom pyspark.sql.functions import col\n\n## @params: [JOB_NAME]\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\n\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\njob.commit()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 19d32ec2-e064-45c4-ac7b-b50e92500cd7.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Current idle_timeout is 2880 minutes.\nidle_timeout has been set to 2880 minutes.\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 19d32ec2-e064-45c4-ac7b-b50e92500cd7.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 3.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 19d32ec2-e064-45c4-ac7b-b50e92500cd7.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous worker type: G.1X\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 19d32ec2-e064-45c4-ac7b-b50e92500cd7.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous number of workers: 10\nSetting new number of workers to: 10\nGlueArgumentError: the following arguments are required: --JOB_NAME\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "output_path = 's3://imba-charlie-test-111/data-partition/orders_partitioned/'\n\n# Load orders from imbadb\norders = glueContext.create_dynamic_frame.from_catalog(database=\"imbadb\", table_name=\"orders\")\norders_df = orders.toDF()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Table `orders` repartitionByRange \n- across user_id\n- 100 bins",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "orders_df.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+-------+--------+------------+---------+-----------------+----------------------+\n|order_id|user_id|eval_set|order_number|order_dow|order_hour_of_day|days_since_prior_order|\n+--------+-------+--------+------------+---------+-----------------+----------------------+\n| 2539329|      1|   prior|           1|        2|                8|                  null|\n| 2398795|      1|   prior|           2|        3|                7|                  15.0|\n|  473747|      1|   prior|           3|        3|               12|                  21.0|\n| 2254736|      1|   prior|           4|        4|                7|                  29.0|\n|  431534|      1|   prior|           5|        4|               15|                  28.0|\n| 3367565|      1|   prior|           6|        2|                7|                  19.0|\n|  550135|      1|   prior|           7|        1|                9|                  20.0|\n| 3108588|      1|   prior|           8|        1|               14|                  14.0|\n| 2295261|      1|   prior|           9|        1|               16|                   0.0|\n| 2550362|      1|   prior|          10|        4|                8|                  30.0|\n| 1187899|      1|   train|          11|        4|                8|                  14.0|\n| 2168274|      2|   prior|           1|        2|               11|                  null|\n| 1501582|      2|   prior|           2|        5|               10|                  10.0|\n| 1901567|      2|   prior|           3|        1|               10|                   3.0|\n|  738281|      2|   prior|           4|        2|               10|                   8.0|\n| 1673511|      2|   prior|           5|        3|               11|                   8.0|\n| 1199898|      2|   prior|           6|        2|                9|                  13.0|\n| 3194192|      2|   prior|           7|        2|               12|                  14.0|\n|  788338|      2|   prior|           8|        1|               15|                  27.0|\n| 1718559|      2|   prior|           9|        2|                9|                   8.0|\n+--------+-------+--------+------------+---------+-----------------+----------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Perform range partitioning on the \"user_id\" column\nmin_user_id = orders_df.selectExpr(\"min(user_id)\").collect()[0][0]\nmax_user_id = orders_df.selectExpr(\"max(user_id)\").collect()[0][0]\nprint(min_user_id, type(min_user_id),max_user_id, type(max_user_id))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "1 <class 'int'> 206209 <class 'int'>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "num_partitions = 100\npartition_size = (max_user_id - min_user_id + 1) // num_partitions\n\norders_df = orders_df.repartitionByRange(\n    num_partitions, col(\"user_id\")\n)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 31,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "orders_df.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+-------+--------+------------+---------+-----------------+----------------------+\n|order_id|user_id|eval_set|order_number|order_dow|order_hour_of_day|days_since_prior_order|\n+--------+-------+--------+------------+---------+-----------------+----------------------+\n| 2539329|      1|   prior|           1|        2|                8|                  null|\n| 2398795|      1|   prior|           2|        3|                7|                  15.0|\n|  473747|      1|   prior|           3|        3|               12|                  21.0|\n| 2254736|      1|   prior|           4|        4|                7|                  29.0|\n|  431534|      1|   prior|           5|        4|               15|                  28.0|\n| 3367565|      1|   prior|           6|        2|                7|                  19.0|\n|  550135|      1|   prior|           7|        1|                9|                  20.0|\n| 3108588|      1|   prior|           8|        1|               14|                  14.0|\n| 2295261|      1|   prior|           9|        1|               16|                   0.0|\n| 2550362|      1|   prior|          10|        4|                8|                  30.0|\n| 1187899|      1|   train|          11|        4|                8|                  14.0|\n| 2168274|      2|   prior|           1|        2|               11|                  null|\n| 1501582|      2|   prior|           2|        5|               10|                  10.0|\n| 1901567|      2|   prior|           3|        1|               10|                   3.0|\n|  738281|      2|   prior|           4|        2|               10|                   8.0|\n| 1673511|      2|   prior|           5|        3|               11|                   8.0|\n| 1199898|      2|   prior|           6|        2|                9|                  13.0|\n| 3194192|      2|   prior|           7|        2|               12|                  14.0|\n|  788338|      2|   prior|           8|        1|               15|                  27.0|\n| 1718559|      2|   prior|           9|        2|                9|                   8.0|\n+--------+-------+--------+------------+---------+-----------------+----------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "orders_df.write.mode(\"overwrite\").parquet(output_path)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 34,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Count the number of records in each partition\ncounts_per_partition = orders_df.rdd.glom().map(len).collect()\nprint(counts_per_partition)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "[33954, 34423, 32636, 32972, 31579, 35339, 32462, 33193, 31087, 35305, 37391, 35356, 34823, 37393, 30778, 37784, 32734, 30672, 33738, 35327, 34930, 34797, 34060, 34951, 34891, 33516, 32953, 35034, 31932, 35362, 34925, 30982, 33951, 34357, 34768, 30555, 35866, 33612, 34079, 31265, 39076, 32663, 34246, 34399, 36448, 35689, 33496, 35059, 34613, 34429, 29920, 35235, 34358, 36361, 31766, 38014, 39583, 32805, 34622, 35542, 33444, 33008, 36674, 35330, 32728, 31840, 33307, 34747, 32670, 34264, 34081, 36593, 32433, 33111, 37975, 36326, 33769, 34832, 34467, 34759, 33539, 32164, 35835, 32049, 36253, 36301, 35869, 33399, 33822, 33832, 30694, 36931, 36421, 33526, 35291, 31939, 33897, 33290, 30954, 34663]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Calculate the average number of records per partition\ntotal_count = sum([count for count in counts_per_partition])\navg_count_per_partition = total_count / 100\nprint(f\"Average number of records per partition: {avg_count_per_partition:.2f}\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 37,
			"outputs": [
				{
					"name": "stdout",
					"text": "Average number of records per partition: 34210.83\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Calculate the standard deviation of the number of records per partition\nimport math\nstd_dev = math.sqrt(sum([(count - avg_count_per_partition) ** 2 for count in counts_per_partition]) / 100)\nprint(f\"Standard deviation of the number of records per partition: {std_dev:.2f}\")\n\n# Calculate the coefficient of variation (CV) of the number of records per partition\ncv = std_dev / avg_count_per_partition\nprint(f\"Coefficient of variation of the number of records per partition: {cv:.2f}\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 38,
			"outputs": [
				{
					"name": "stdout",
					"text": "Standard deviation of the number of records per partition: 1916.87\nCoefficient of variation of the number of records per partition: 0.06\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import countDistinct\n\ndistinct_user_ids = orders_df.select(countDistinct(\"user_id\")).collect()[0][0]\nprint(\"Distinct user IDs:\", distinct_user_ids)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 39,
			"outputs": [
				{
					"name": "stdout",
					"text": "Distinct user IDs: 206209\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Table `order_product` repartitionByRange\n- across product_id\n- 50 bins",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Load order_product from imbadb\noutput_path = 's3://imba-charlie-test-111/data-partition/op_partitioned/'\nop = glueContext.create_dynamic_frame.from_catalog(database=\"imbadb\", table_name=\"order_product\")\nop_df = op.toDF()",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "print(\"Hello\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 41,
			"outputs": [
				{
					"name": "stdout",
					"text": "Hello\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Table `pda` repartition\n- merged table of products, departements, aisles,\n- across department_id (21 distinct values)",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "products_df = glueContext.create_dynamic_frame.from_catalog(database=\"imbadb\", table_name=\"products\")\ndepartments_df = glueContext.create_dynamic_frame.from_catalog(database=\"imbadb\", table_name=\"departments\")\naisles_df = glueContext.create_dynamic_frame.from_catalog(database=\"imbadb\", table_name=\"aisles\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 47,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Convert the dynamic frames to data frames\nproducts_df = products_df.toDF()\ndepartments_df = departments_df.toDF()\naisles_df = aisles_df.toDF()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 48,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# register DataFrames as temporary views\nproducts_df.createOrReplaceTempView(\"products\")\ndepartments_df.createOrReplaceTempView(\"departments\")\naisles_df.createOrReplaceTempView(\"aisles\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 51,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "pda_df = spark.sql(\"\"\"\n    SELECT p.product_id, p.product_name, p.department_id, d.department, p.aisle_id, a.aisle\n    FROM products p\n    LEFT JOIN departments d ON p.department_id = d.department_id\n    LEFT JOIN aisles a ON p.aisle_id = a.aisle_id\n\"\"\")\n\npda_df.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 61,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+--------------------+-------------+----------+--------+--------------------+\n|product_id|        product_name|department_id|department|aisle_id|               aisle|\n+----------+--------------------+-------------+----------+--------+--------------------+\n|        14|Fresh Scent Dishw...|           17| household|      74|     dish detergents|\n|        48|School Glue Washa...|           17| household|      87|      more household|\n|        57|     Flat Toothpicks|           17| household|     111|plates bowls cups...|\n|        71|Ultra 7 Inch Poly...|           17| household|     111|plates bowls cups...|\n|       105|Easy Grab 9x13 Ob...|           17| household|      10|    kitchen supplies|\n|       111|Fabric Softener G...|           17| household|      75|             laundry|\n|       134|Stain Release Boo...|           17| household|      75|             laundry|\n|       153|Fabric Refresher ...|           17| household|      75|             laundry|\n|       186|Fresh Scent Dishw...|           17| household|      74|     dish detergents|\n|       204|Free & Clear Natu...|           17| household|      75|             laundry|\n|       224|Foaming Hand Soap...|           17| household|     114|   cleaning products|\n|       246|Triple-Acting Lau...|           17| household|      75|             laundry|\n|       265|Glass Storage Con...|           17| household|      85|        food storage|\n|       268|          Sponge Mop|           17| household|     114|   cleaning products|\n|       270|OneZip Storage Ba...|           17| household|      85|        food storage|\n|       274|          Snack Bags|           17| household|      87|      more household|\n|       318|Apple Cinnamon Cr...|           17| household|     101|air fresheners ca...|\n|       328|Disinfectant Spra...|           17| household|     101|air fresheners ca...|\n|       354|Twist n Loc Small...|           17| household|      85|        food storage|\n|       368|Fabric Softener D...|           17| household|      75|             laundry|\n+----------+--------------------+-------------+----------+--------+--------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "output_path_pda = 's3://imba-charlie-test-111/data-partition/pda_partitioned/'\npda_df.write.mode(\"overwrite\").partitionBy(\"department_id\").parquet(output_path_pda)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 62,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}