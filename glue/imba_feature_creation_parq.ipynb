{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom datetime import datetime\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.0 \nCurrent idle_timeout is 2800 minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 5\nSetting new number of workers to: 5\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::117760591469:role/AWSGlueServiceRoleDefault\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: 14b95ceb-de2b-4d57-b25f-ac4554473547\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.0\n--enable-glue-datacatalog true\nWaiting for session 14b95ceb-de2b-4d57-b25f-ac4554473547 to get into ready status...\nSession 14b95ceb-de2b-4d57-b25f-ac4554473547 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Define parameters\nnow = datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\nbucket = 'jrde-upload-from-glue'\nfolder = 'query-results-' + now\n\n# Table name in data catalog\ndb = 'from_rds'\naisles_table = 'ecommerce_public_aisles'\ndepartments_table = 'ecommerce_public_departments'\norders_table = 'ecommerce_public_orders'\nproducts_table = 'ecommerce_public_products'\norder_product_table = 'ecommerce_public_order_product'",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Create dataframes for raw datasets\naisles_df = glueContext.create_dynamic_frame.from_catalog(database=db, table_name=aisles_table).toDF()\ndepartments_df = glueContext.create_dynamic_frame.from_catalog(database=db, table_name=departments_table).toDF()\norders_df = glueContext.create_dynamic_frame.from_catalog(database=db, table_name=orders_table).toDF()\nproducts_df = glueContext.create_dynamic_frame.from_catalog(database=db, table_name=products_table).toDF()\norder_product_df = glueContext.create_dynamic_frame.from_catalog(database=db, table_name=order_product_table).toDF()",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Create order_products_prior view for getting new data\nprior_orders = orders_df.filter(orders_df['eval_set']=='prior')\njoined_order_product = prior_orders.join(order_product_df, 'order_id', 'left')\njoined_order_product.createOrReplaceTempView('order_products_prior')\norders_df.createOrReplaceTempView('orders_view')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Query 1\ntable1 = spark.sql('select user_id, \\n'\n                          'max(order_number) max_order_number, \\n'\n                          'sum(day_since_prior_order) sum_day_since_prior_order, \\n'\n                          'avg(day_since_prior_order) avg_day_since_prior_order \\n'\n                   'from orders_view \\n'\n                   'group by user_id;')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Query 2\ntable2 = spark.sql('select user_id, \\n'\n                          'count(product_id) total_products_groupby_userid, \\n'\n                          'count(distinct(product_id)) distinct_products, \\n'\n                          'sum(reordered) / count(case when order_number > 1 then 1 else 0 end) user_reorder_ratio \\n'\n                   'from order_products_prior \\n'\n                   'group by user_id;')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Query 3\ntable3 = spark.sql('select user_id, \\n'\n                          'product_id, \\n'\n                          'sum(order_number) total_products_groupby_userid_productid, \\n'\n                          'min(order_number) min_ordernum_groupby_userid_productid, \\n'\n                          'max(order_number) max_ordernum_groupby_userid_productid, \\n'\n                          'avg(add_to_cart_order) avg_addtocartorder_groupby_userid_productid \\n'\n                   'from order_products_prior \\n'\n                   'group by user_id, product_id;')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Query 4\ntable4 = spark.sql('with temp as ( \\n'\n                    'select user_id, \\n'\n                    '       order_number, \\n'\n                    '       product_id, \\n'\n                    '       reordered, \\n'\n                    '       rank() over(partition by user_id, product_id order by order_number) product_seq_time \\n'\n                    'from order_products_prior \\n'\n                  ') \\n'\n                  'select product_id, \\n'\n                         'count(product_id) total_products_groupby_productid, \\n'\n                         'sum(reordered) total_reordered, \\n'\n                         'sum(case when product_seq_time = 1 then 1 else 0 end) product_seq_time_1, \\n'\n                         'sum(case when product_seq_time = 2 then 1 else 0 end) product_seq_time_2 \\n'\n                  'from temp \\n'\n                  'group by product_id;')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Join all tables above\nderived_table = table1.join(table2, 'user_id', 'left')\nderived_table = derived_table.join(table3, 'user_id', 'left')\nderived_table = derived_table.join(table4, 'product_id', 'left')\n\n# Save dataframe to S3\nderived_table.coalesce(1).write.option('header', 'true').mode('overwrite').csv(f's3://{bucket}/{folder}/')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		}
	]
}